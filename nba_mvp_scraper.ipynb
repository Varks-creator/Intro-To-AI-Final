{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f221551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in ./.venv/lib/python3.13/site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in ./.venv/lib/python3.13/site-packages (4.13.4)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.13/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.13/site-packages (2.2.5)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.13/site-packages (4.67.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests) (2025.4.26)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./.venv/lib/python3.13/site-packages (from beautifulsoup4) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in ./.venv/lib/python3.13/site-packages (from beautifulsoup4) (4.13.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 pandas numpy tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f70b768b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b84089e3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Set up logging with more detailed format\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b815c94",
   "metadata": {},
   "source": [
    "# Scraping Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97233189",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import logging\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Optional: configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "class NBADataScraper:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://www.basketball-reference.com\"\n",
    "        self.headers = {\n",
    "            'User-Agent': (\n",
    "                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\n",
    "                'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                'Chrome/91.0.4472.124 Safari/537.36'\n",
    "            )\n",
    "        }\n",
    "\n",
    "    def get_page_content(self, url):\n",
    "        \"\"\"Fetch page content with error handling and rate limiting.\"\"\"\n",
    "        try:\n",
    "            logging.info(f\"Fetching URL: {url}\")\n",
    "            time.sleep(3)\n",
    "            resp = requests.get(url, headers=self.headers)\n",
    "            resp.raise_for_status()\n",
    "            return BeautifulSoup(resp.content, 'html.parser')\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Request error for {url}: {e}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error fetching {url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_mvp_winner(self, year):\n",
    "        \"\"\"Extract the MVP winner’s name for a given season.\"\"\"\n",
    "        url  = f\"{self.base_url}/awards/awards_{year}.html\"\n",
    "        soup = self.get_page_content(url)\n",
    "        if not soup:\n",
    "            return None\n",
    "\n",
    "        table = soup.find('table', {'id': 'mvp'})\n",
    "        if not table:\n",
    "            logging.error(f\"No MVP table for {year}\")\n",
    "            return None\n",
    "\n",
    "        row = table.find('tbody').find('tr')\n",
    "        if not row:\n",
    "            logging.error(f\"No MVP winner row for {year}\")\n",
    "            return None\n",
    "\n",
    "        return row.find('td', {'data-stat': 'player'}).text.strip()\n",
    "\n",
    "    def get_mvp_shares(self, year):\n",
    "        \"\"\"\n",
    "        Extract the full MVP voting table for `year` and return\n",
    "        a dict mapping player → share (float).\n",
    "        \"\"\"\n",
    "        url  = f\"{self.base_url}/awards/awards_{year}.html\"\n",
    "        soup = self.get_page_content(url)\n",
    "        if not soup:\n",
    "            return {}\n",
    "\n",
    "        table = soup.find('table', {'id': 'mvp'})\n",
    "        if not table:\n",
    "            logging.error(f\"No MVP table for {year}\")\n",
    "            return {}\n",
    "\n",
    "        df = pd.read_html(str(table))[0]\n",
    "        # flatten if pandas created a MultiIndex\n",
    "        if isinstance(df.columns, pd.MultiIndex):\n",
    "            df.columns = df.columns.droplevel(0)\n",
    "\n",
    "        df['Player'] = df['Player'].str.replace(r\"\\*+\", \"\", regex=True).str.strip()\n",
    "        df['Share']  = pd.to_numeric(df['Share'], errors='coerce').fillna(0.0)\n",
    "\n",
    "        return dict(zip(df['Player'], df['Share']))\n",
    "\n",
    "    def get_basic_stats(self, year):\n",
    "        \"\"\"Extract per‑game stats for `year`.\"\"\"\n",
    "        url  = f\"{self.base_url}/leagues/NBA_{year}_per_game.html\"\n",
    "        soup = self.get_page_content(url)\n",
    "        if not soup:\n",
    "            return None\n",
    "\n",
    "        table = soup.find('table', {'id': 'per_game_stats'})\n",
    "        if not table:\n",
    "            logging.error(f\"No basic stats table for {year}\")\n",
    "            return None\n",
    "\n",
    "        df = pd.read_html(str(table))[0]\n",
    "        # flatten any MultiIndex\n",
    "        if isinstance(df.columns, pd.MultiIndex):\n",
    "            df.columns = df.columns.droplevel(0)\n",
    "\n",
    "        df = df[df['Player'].notna()]\n",
    "        df = df[~df['Player'].str.contains('Player')]\n",
    "\n",
    "        col_map = {\n",
    "            'Tm': 'Team', 'Pos': 'Position', 'G': 'Games', 'MP': 'Minutes',\n",
    "            'PTS': 'Points', 'TRB': 'Rebounds', 'AST': 'Assists',\n",
    "            'STL': 'Steals', 'BLK': 'Blocks', 'TOV': 'Turnovers',\n",
    "            'FG%': 'FG_Pct', '3P%': '3P_Pct', 'FT%': 'FT_Pct'\n",
    "        }\n",
    "        for old, new in col_map.items():\n",
    "            if old in df.columns:\n",
    "                df = df.rename(columns={old: new})\n",
    "\n",
    "        wanted = [\n",
    "            'Player','Team','Position','Season','Games','Minutes','Points',\n",
    "            'Rebounds','Assists','Steals','Blocks','Turnovers',\n",
    "            'FG_Pct','3P_Pct','FT_Pct'\n",
    "        ]\n",
    "        available = [c for c in wanted if c in df.columns]\n",
    "        return df[available]\n",
    "\n",
    "    def get_advanced_stats(self, year):\n",
    "        \"\"\"Extract advanced stats for `year`.\"\"\"\n",
    "        url  = f\"{self.base_url}/leagues/NBA_{year}_advanced.html\"\n",
    "        soup = self.get_page_content(url)\n",
    "        if not soup:\n",
    "            return None\n",
    "\n",
    "        table = soup.find('table', {'class': 'stats_table'})\n",
    "        if not table:\n",
    "            logging.error(f\"No advanced stats table for {year}\")\n",
    "            return None\n",
    "\n",
    "        df = pd.read_html(str(table))[0]\n",
    "        # flatten any MultiIndex\n",
    "        if isinstance(df.columns, pd.MultiIndex):\n",
    "            df.columns = df.columns.droplevel(0)\n",
    "\n",
    "        df = df[df['Player'].notna()]\n",
    "        df = df[~df['Player'].str.contains('Player')]\n",
    "\n",
    "        col_map = {\n",
    "            'PER':'Player_Efficiency_Rating','WS':'Win_Shares',\n",
    "            'BPM':'Box_Plus_Minus','USG%':'Usage_Rate','VORP':'Value_Over_Replacement',\n",
    "            'WS/48':'Win_Shares_Per_48'\n",
    "        }\n",
    "        for old, new in col_map.items():\n",
    "            if old in df.columns:\n",
    "                df = df.rename(columns={old: new})\n",
    "\n",
    "        wanted = [\n",
    "            'Player','Player_Efficiency_Rating','Win_Shares',\n",
    "            'Box_Plus_Minus','Usage_Rate','Value_Over_Replacement',\n",
    "            'Win_Shares_Per_48'\n",
    "        ]\n",
    "        available = [c for c in wanted if c in df.columns]\n",
    "        return df[available]\n",
    "\n",
    "    def scrape_season(self, year):\n",
    "        \"\"\"Combine basic, advanced, MVP‑winner and vote‐share into one DF.\"\"\"\n",
    "        logging.info(f\"Scraping season {year}…\")\n",
    "\n",
    "        winner       = self.get_mvp_winner(year)\n",
    "        basic        = self.get_basic_stats(year)\n",
    "        advanced     = self.get_advanced_stats(year)\n",
    "        shares_lookup= self.get_mvp_shares(year)\n",
    "\n",
    "        if winner is None or basic is None or advanced is None:\n",
    "            logging.error(f\"Skipping {year}: missing data\")\n",
    "            return None\n",
    "\n",
    "        df = pd.merge(basic, advanced, on='Player', how='left')\n",
    "        df['Season'] = year\n",
    "        df['MVP']    = (df['Player'] == winner).astype(int)\n",
    "        df['Share']  = df['Player'].map(shares_lookup).fillna(0.0)\n",
    "\n",
    "        logging.info(f\"Finished {year}: {len(df)} players\")\n",
    "        return df\n",
    "\n",
    "    def scrape_all_seasons(self, start_year=1981, end_year=2024):\n",
    "        \"\"\"Loop through seasons and concatenate into one DataFrame.\"\"\"\n",
    "        all_seasons = []\n",
    "        for yr in tqdm(range(start_year, end_year+1), desc=\"Seasons\"):\n",
    "            season_df = self.scrape_season(yr)\n",
    "            if season_df is not None:\n",
    "                all_seasons.append(season_df)\n",
    "\n",
    "        if not all_seasons:\n",
    "            logging.error(\"No seasons scraped!\")\n",
    "            return None\n",
    "\n",
    "        final = pd.concat(all_seasons, ignore_index=True)\n",
    "        final = final.fillna(0)\n",
    "        num_cols = [\n",
    "            'Minutes','Points','Rebounds','Assists','Steals','Blocks','Turnovers',\n",
    "            'FG_Pct','3P_Pct','FT_Pct','Player_Efficiency_Rating','Win_Shares',\n",
    "            'Box_Plus_Minus','Usage_Rate','Value_Over_Replacement',\n",
    "            'Win_Shares_Per_48','Share'\n",
    "        ]\n",
    "        for c in num_cols:\n",
    "            if c in final.columns:\n",
    "                final[c] = pd.to_numeric(final[c], errors='coerce')\n",
    "\n",
    "        return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fc76bc3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    scraper = NBADataScraper()\n",
    "    final_df = scraper.scrape_all_seasons()\n",
    "    \n",
    "    if final_df is not None:\n",
    "        # Save to CSV\n",
    "        output_file = 'nba_mvp_data_NEW_TESTTTT.csv'\n",
    "        final_df.to_csv(output_file, index=False)\n",
    "        logging.info(f\"Data successfully saved to {output_file}\")\n",
    "        \n",
    "        # Print some basic statistics about the dataset\n",
    "        logging.info(f\"\\nDataset Statistics:\")\n",
    "        logging.info(f\"Total number of player-seasons: {len(final_df)}\")\n",
    "        logging.info(f\"Number of MVP winners: {final_df['MVP'].sum()}\")\n",
    "        logging.info(f\"Seasons covered: {final_df['Season'].min()} to {final_df['Season'].max()}\")\n",
    "    else:\n",
    "        logging.error(\"Failed to create the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d595a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seasons:   0%|          | 0/44 [00:00<?, ?it/s]2025-05-10 20:18:41,908 - INFO - Scraping season 1981…\n",
      "2025-05-10 20:18:41,912 - INFO - Fetching URL: https://www.basketball-reference.com/awards/awards_1981.html\n",
      "2025-05-10 20:18:45,051 - INFO - Fetching URL: https://www.basketball-reference.com/leagues/NBA_1981_per_game.html\n",
      "/var/folders/sh/qwxdyjvj1msdxcfnkpk9nz6r0000gn/T/ipykernel_63170/1286478929.py:96: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n",
      "2025-05-10 20:18:48,847 - INFO - Fetching URL: https://www.basketball-reference.com/leagues/NBA_1981_advanced.html\n",
      "/var/folders/sh/qwxdyjvj1msdxcfnkpk9nz6r0000gn/T/ipykernel_63170/1286478929.py:134: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n",
      "2025-05-10 20:18:52,338 - INFO - Fetching URL: https://www.basketball-reference.com/awards/awards_1981.html\n",
      "/var/folders/sh/qwxdyjvj1msdxcfnkpk9nz6r0000gn/T/ipykernel_63170/1286478929.py:74: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n",
      "2025-05-10 20:18:55,468 - INFO - Finished 1981: 546 players\n",
      "Seasons:   2%|▏         | 1/44 [00:13<09:43, 13.56s/it]2025-05-10 20:18:55,469 - INFO - Scraping season 1982…\n",
      "2025-05-10 20:18:55,469 - INFO - Fetching URL: https://www.basketball-reference.com/awards/awards_1982.html\n",
      "2025-05-10 20:18:58,838 - INFO - Fetching URL: https://www.basketball-reference.com/leagues/NBA_1982_per_game.html\n",
      "/var/folders/sh/qwxdyjvj1msdxcfnkpk9nz6r0000gn/T/ipykernel_63170/1286478929.py:96: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n",
      "2025-05-10 20:19:02,351 - INFO - Fetching URL: https://www.basketball-reference.com/leagues/NBA_1982_advanced.html\n",
      "/var/folders/sh/qwxdyjvj1msdxcfnkpk9nz6r0000gn/T/ipykernel_63170/1286478929.py:134: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n",
      "2025-05-10 20:19:05,924 - INFO - Fetching URL: https://www.basketball-reference.com/awards/awards_1982.html\n",
      "/var/folders/sh/qwxdyjvj1msdxcfnkpk9nz6r0000gn/T/ipykernel_63170/1286478929.py:74: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n",
      "2025-05-10 20:19:09,050 - INFO - Finished 1982: 552 players\n",
      "Seasons:   5%|▍         | 2/44 [00:27<09:30, 13.57s/it]2025-05-10 20:19:09,050 - INFO - Scraping season 1983…\n",
      "2025-05-10 20:19:09,050 - INFO - Fetching URL: https://www.basketball-reference.com/awards/awards_1983.html\n",
      "2025-05-10 20:19:12,181 - INFO - Fetching URL: https://www.basketball-reference.com/leagues/NBA_1983_per_game.html\n",
      "/var/folders/sh/qwxdyjvj1msdxcfnkpk9nz6r0000gn/T/ipykernel_63170/1286478929.py:96: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n",
      "2025-05-10 20:19:15,718 - INFO - Fetching URL: https://www.basketball-reference.com/leagues/NBA_1983_advanced.html\n",
      "/var/folders/sh/qwxdyjvj1msdxcfnkpk9nz6r0000gn/T/ipykernel_63170/1286478929.py:134: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n",
      "2025-05-10 20:19:19,458 - INFO - Fetching URL: https://www.basketball-reference.com/awards/awards_1983.html\n",
      "/var/folders/sh/qwxdyjvj1msdxcfnkpk9nz6r0000gn/T/ipykernel_63170/1286478929.py:74: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n",
      "2025-05-10 20:19:22,586 - INFO - Finished 1983: 630 players\n",
      "Seasons:   7%|▋         | 3/44 [00:40<09:15, 13.56s/it]2025-05-10 20:19:22,587 - INFO - Scraping season 1984…\n",
      "2025-05-10 20:19:22,587 - INFO - Fetching URL: https://www.basketball-reference.com/awards/awards_1984.html\n",
      "2025-05-10 20:19:25,758 - INFO - Fetching URL: https://www.basketball-reference.com/leagues/NBA_1984_per_game.html\n",
      "/var/folders/sh/qwxdyjvj1msdxcfnkpk9nz6r0000gn/T/ipykernel_63170/1286478929.py:96: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n",
      "2025-05-10 20:19:29,323 - INFO - Fetching URL: https://www.basketball-reference.com/leagues/NBA_1984_advanced.html\n",
      "Seasons:   7%|▋         | 3/44 [00:50<11:27, 16.77s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m():\n\u001b[32m      2\u001b[39m     scraper = NBADataScraper()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     final_df = \u001b[43mscraper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscrape_all_seasons\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m final_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m      6\u001b[39m         \u001b[38;5;66;03m# Save to CSV\u001b[39;00m\n\u001b[32m      7\u001b[39m         output_file = \u001b[33m'\u001b[39m\u001b[33mnba_mvp_data_NEW_TESTTTT.csv\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 184\u001b[39m, in \u001b[36mNBADataScraper.scrape_all_seasons\u001b[39m\u001b[34m(self, start_year, end_year)\u001b[39m\n\u001b[32m    182\u001b[39m all_seasons = []\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m yr \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(start_year, end_year+\u001b[32m1\u001b[39m), desc=\u001b[33m\"\u001b[39m\u001b[33mSeasons\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m     season_df = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscrape_season\u001b[49m\u001b[43m(\u001b[49m\u001b[43myr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m season_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    186\u001b[39m         all_seasons.append(season_df)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 165\u001b[39m, in \u001b[36mNBADataScraper.scrape_season\u001b[39m\u001b[34m(self, year)\u001b[39m\n\u001b[32m    163\u001b[39m winner       = \u001b[38;5;28mself\u001b[39m.get_mvp_winner(year)\n\u001b[32m    164\u001b[39m basic        = \u001b[38;5;28mself\u001b[39m.get_basic_stats(year)\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m advanced     = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_advanced_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m shares_lookup= \u001b[38;5;28mself\u001b[39m.get_mvp_shares(year)\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m winner \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m basic \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m advanced \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 125\u001b[39m, in \u001b[36mNBADataScraper.get_advanced_stats\u001b[39m\u001b[34m(self, year)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Extract advanced stats for `year`.\"\"\"\u001b[39;00m\n\u001b[32m    124\u001b[39m url  = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.base_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/leagues/NBA_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_advanced.html\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m soup = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_page_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m soup:\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mNBADataScraper.get_page_content\u001b[39m\u001b[34m(self, url)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     28\u001b[39m     logging.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFetching URL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m     resp = requests.get(url, headers=\u001b[38;5;28mself\u001b[39m.headers)\n\u001b[32m     31\u001b[39m     resp.raise_for_status()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bd0cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('nba_mvp_data_NEW_TESTTTT.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccd760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9344ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mvplist = data.loc[data.groupby('Season')['Share'].idxmax(), ['Player','Season']]\n",
    "mvplist = mvplist.assign(is_mvp=True)\n",
    "\n",
    "data = data.merge(mvplist, on=['Player','Season'], how='left')\n",
    "data['is_mvp'] = data['is_mvp'].fillna(False).astype(bool)\n",
    "\n",
    "print(data['is_mvp'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed99376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Narrow data set to those who played at least 30 games, averaged more than 10 minutes, and scored at least 5 points\n",
    "\n",
    "data = data[(data['Games'] >= 30) & (data['Minutes'] > 10) & (data['Points'] > 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeb0ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['MVP'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cbfce3",
   "metadata": {},
   "source": [
    "# Determining Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9448efc",
   "metadata": {},
   "source": [
    "We can use a correlation matrix to see how impactful each column in the table is relative to determining MVP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83df630c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unecessary columns \n",
    "\n",
    "mvpfactors = data.copy()\n",
    "mvpfactors\n",
    "\n",
    "# list the players who have won the MVP award\n",
    "mvpfactors[mvpfactors['MVP'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8537ee45",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec85ab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90d0608",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modeling different stats and their correlation to winning MVP\n",
    "# - First, looking at Value Over Replacement to Award Shares\n",
    "plt.figure(figsize = (10,6))\n",
    "plt.scatter(data['Value_Over_Replacement'], data['Share'])\n",
    "plt.title('MVP Award Shares vs. Value Over Replacement')\n",
    "plt.xlabel('Vorp')\n",
    "plt.ylabel('Award Shares')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad704229",
   "metadata": {},
   "outputs": [],
   "source": [
    "mvpfactors.drop(columns=['Share', 'Team', 'Player', 'MVP', 'Season', 'Position'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dbd8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corrmatrix = mvpfactors.corr()\n",
    "\n",
    "\n",
    "# get top 20 factors that impact mvp\n",
    "# mvp is right now yes(1) or no(0)\n",
    "top_20_factors = corrmatrix['is_mvp'].abs().sort_values(ascending=False).head(20).index\n",
    "\n",
    "top_20_factors\n",
    "\n",
    "corrmatrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7597805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b15620f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c3f956",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_corr_matrix = corrmatrix.loc[top_20_factors, top_20_factors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0201ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))  # Adjust size for clarity\n",
    "sns.heatmap(\n",
    "    sorted_corr_matrix,\n",
    "    vmin=-1, vmax=1,\n",
    "    cmap=\"ocean\",\n",
    "    center=0,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    annot_kws={\"fontsize\": 8},\n",
    "    linewidths=0.5,\n",
    "    linecolor=\"white\",\n",
    "    cbar=True,\n",
    "    cbar_kws={\"orientation\": \"vertical\"},\n",
    "    square=True,\n",
    "    xticklabels=True,\n",
    "    yticklabels=True,\n",
    "    ax=ax\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594321de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictors by taking the top 20 factors\n",
    "\n",
    "predictors = sorted_corr_matrix.index[:21]\n",
    "predictors = predictors[predictors != 'is_mvp']\n",
    "predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57360d34",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce11a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018c281b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "\n",
    "# ─── 2) Split train/test by season ────────────────────────────────────────────\n",
    "test_years = [2021, 2022, 2023, 2024]\n",
    "\n",
    "train = data[~data['Season'].isin(test_years)]\n",
    "test  = data[ data['Season'].isin(test_years)]\n",
    "\n",
    "X_train = train[predictors]\n",
    "X_test  = test[predictors]\n",
    "\n",
    "y_train_mvp   = train['MVP']    # binary 0/1\n",
    "y_test_mvp    = test['MVP']\n",
    "y_train_share = train['Share']  # continuous 0–1\n",
    "y_test_share  = test['Share']\n",
    "\n",
    "print(f\"Training on {len(train)} rows; testing on {len(test)} rows.\\n\")\n",
    "\n",
    "# ─── 3) Logistic Regression for MVP (0/1) ─────────────────────────────────────\n",
    "log_model = LogisticRegression(max_iter=1000)\n",
    "log_model.fit(X_train, y_train_mvp)\n",
    "\n",
    "print(\"— Logistic Regression (MVP) —\")\n",
    "for yr in test_years:\n",
    "    sub = test[test['Season'] == yr]\n",
    "    y_true = sub['MVP']\n",
    "    y_proba = log_model.predict_proba(sub[predictors])[:,1]\n",
    "    auc = roc_auc_score(y_true, y_proba)\n",
    "    # top‑1 hit?\n",
    "    top1    = int(sub.iloc[y_proba.argmax()]['MVP'])\n",
    "    print(f\"{yr}  ROC‑AUC: {auc:.3f}, Top‑1 Acc: {top1}\")\n",
    "\n",
    "# overall classification report on all four seasons combined:\n",
    "y_pred_all = log_model.predict(X_test)\n",
    "y_proba_all = log_model.predict_proba(X_test)[:,1]\n",
    "print(\"\\nCombined classification report:\")\n",
    "print(classification_report(y_test_mvp, y_pred_all))\n",
    "print(f\"Overall accuracy: {accuracy_score(y_test_mvp, y_pred_all):.3f}\")\n",
    "print(f\"Overall ROC‑AUC : {roc_auc_score(y_test_mvp, y_proba_all):.3f}\")\n",
    "\n",
    "# ─── 4) Linear Regression for MVP Vote Share ─────────────────────────────────────\n",
    "lin_model = LinearRegression()\n",
    "lin_model.fit(X_train, y_train_share)\n",
    "\n",
    "y_pred_share = lin_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test_share, y_pred_share)\n",
    "r2  = r2_score(y_test_share, y_pred_share)\n",
    "\n",
    "print(\"\\n— Linear Regression (Vote Share) —\")\n",
    "print(f\"Test MSE: {mse:.4f}\")\n",
    "print(f\"Test RMSE: {np.sqrt(mse):.4f}\")\n",
    "print(f\"Test R² : {r2:.4f}\")\n",
    "\n",
    "# ─── 5) Feature Importance ───────────────────────────────────────────────────\n",
    "coef_log   = pd.Series(log_model.coef_[0], index=predictors)\n",
    "coef_lin   = pd.Series(lin_model.coef_,    index=predictors)\n",
    "\n",
    "print(\"\\nTop 10 logistic coefficients (|β| for MVP):\")\n",
    "print(coef_log.abs().sort_values(ascending=False).head(10))\n",
    "\n",
    "print(\"\\nTop 10 linear coefficients (|β| for Share):\")\n",
    "print(coef_lin.abs().sort_values(ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4252b34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_years= [2021, 2022, 2023, 2024]\n",
    "test= data[data['Season'].isin(test_years)].copy()\n",
    "test['predicted_share'] = lin_model.predict(test[predictors])\n",
    "\n",
    "# 2) For each year, sort & show top 10\n",
    "for yr in test_years:\n",
    "    sub = test[test['Season'] == yr]\n",
    "    top10 = (\n",
    "        sub[['Player','Share','predicted_share']]\n",
    "        .sort_values('predicted_share', ascending=False)\n",
    "        .head(10)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    print(f\"\\n=== Top 10 Predicted Vote Shares for {yr} ===\")\n",
    "    print(top10.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc197017",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub2023 = test[test['Season']==2023]\n",
    "proba2023 = log_model.predict_proba(sub2023[predictors])[:,1]\n",
    "print(sub2023.assign(proba=proba2023)\n",
    "              .sort_values('proba', ascending=False)\n",
    "              .head(5)[['Player','proba','MVP']])\n",
    "\n",
    "print('------')\n",
    "\n",
    "sub2024 = test[test['Season']==2024]\n",
    "proba2024 = log_model.predict_proba(sub2024[predictors])[:,1]\n",
    "print(sub2024.assign(proba=proba2024)\n",
    "              .sort_values('proba', ascending=False)\n",
    "              .head(5)[['Player','proba','MVP']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73599ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in (1,3,5):\n",
    "    hits = []\n",
    "    for yr in test_years:\n",
    "        sub = test[test['Season']==yr]\n",
    "        proba = model.predict_proba(sub[predictors])[:,1]\n",
    "        topk = sub.iloc[proba.argsort()[-k:]]['MVP']\n",
    "        hits.append(topk.any())\n",
    "    print(f\"Top-{k} accuracy:\", sum(hits)/len(hits))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daebde9f",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a3bf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import LeaveOneGroupOut, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 1) Re‑build your train/test split\n",
    "test_years = [2021, 2022, 2023, 2024]\n",
    "train = data[~data['Season'].isin(test_years)]\n",
    "test  = data[ data['Season'].isin(test_years)]\n",
    "\n",
    "X_train, y_train = train[predictors], train['Share']\n",
    "X_test,  y_test  = test[predictors],  test['Share']\n",
    "\n",
    "# 2) Set up Leave‑One‑Season‑Out\n",
    "logo   = LeaveOneGroupOut()\n",
    "groups = train['Season'].values\n",
    "\n",
    "# 3) Grid‑search over alpha with group CV\n",
    "param_grid = {'alpha': [0.01, 0.1, 1.0, 10.0, 100.0]}\n",
    "grid = GridSearchCV(\n",
    "    Ridge(), \n",
    "    param_grid,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=logo.split(X_train, y_train, groups),\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best α (by L-OO-CV MSE):\", grid.best_params_['alpha'])\n",
    "\n",
    "# 4) Fit final Ridge with that α\n",
    "ridge = Ridge(alpha=grid.best_params_['alpha'])\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# 5) Evaluate on your 2021–24 hold‑out\n",
    "y_pred = ridge.predict(X_test)\n",
    "print(\"Hold‑out MSE:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"Hold‑out R² :\", r2_score(y_test, y_pred))\n",
    "\n",
    "# 6) Top features\n",
    "coef = pd.Series(ridge.coef_, index=predictors).abs().sort_values(ascending=False)\n",
    "print(\"\\nTop 10 features by |coef|:\\n\", coef.head(10))\n",
    "\n",
    "# 7) Per‑year “Top‑10 predicted share” breakdown\n",
    "for yr in test_years:\n",
    "    sub = test[test['Season'] == yr].copy()\n",
    "    sub['predicted_share'] = ridge.predict(sub[predictors])\n",
    "    top10 = sub[['Player','Share','predicted_share']]\\\n",
    "            .sort_values('predicted_share', ascending=False)\\\n",
    "            .head(10)\\\n",
    "            .reset_index(drop=True)\n",
    "    print(f\"\\n=== Top 10 Ridge‑predicted Vote Shares for {yr} ===\")\n",
    "    print(top10.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4b48eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Add back Player, Season, and actual vote share\n",
    "results = test.copy().loc[:, ['Player','Season','Share']].rename(columns={'Share':'Award_Shares'})\n",
    "\n",
    "# 2) Predict vote shares for the 2022 slice only\n",
    "slice_2022 = results[results['Season']==2024].copy()\n",
    "slice_2022['predictions'] = ridge.predict(test.loc[slice_2022.index, predictors])\n",
    "\n",
    "# 3) Compute the true rank (Rk) by Award_Shares\n",
    "slice_2022 = slice_2022.sort_values('Award_Shares', ascending=False)\n",
    "slice_2022['Rank'] = range(1, len(slice_2022)+1)\n",
    "\n",
    "# 4) Compute the predicted rank (Predicted_Rk)\n",
    "slice_2022 = slice_2022.sort_values('predictions', ascending=False)\n",
    "slice_2022['Predicted_Rank'] = range(1, len(slice_2022)+1)\n",
    "\n",
    "# 5) Re‑sort by actual vote share to display\n",
    "combination_2022 = slice_2022.sort_values('Rank').reset_index(drop=True)\n",
    "\n",
    "print(combination_2022.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbefda95",
   "metadata": {},
   "source": [
    "# Random Forest + GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fea3d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.metrics import classification_report, roc_auc_score, mean_squared_error, r2_score\n",
    "\n",
    "# ─── 1) RandomForest for MVP classification ────────────────────────────────\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    random_state=42,\n",
    "    class_weight='balanced'   # helps with the 1-vs-many imbalance\n",
    ")\n",
    "rf_clf.fit(X_train, y_train_mvp)\n",
    "\n",
    "y_pred_rf    = rf_clf.predict(X_test)\n",
    "y_proba_rf   = rf_clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(\"\\n— RandomForestClassifier (MVP) —\")\n",
    "print(classification_report(y_test_mvp, y_pred_rf))\n",
    "print(\"ROC‑AUC:\", roc_auc_score(y_test_mvp, y_proba_rf))\n",
    "\n",
    "# ─── 3) RandomForest for vote‑share regression ─────────────────────────────\n",
    "rf_reg = RandomForestRegressor(\n",
    "    n_estimators=200,\n",
    "    random_state=42\n",
    ")\n",
    "rf_reg.fit(X_train, y_train_share)\n",
    "\n",
    "y_pred_rf_reg = rf_reg.predict(X_test)\n",
    "\n",
    "print(\"\\n— RandomForestRegressor (Share) —\")\n",
    "print(\"MSE:\", mean_squared_error(y_test_share, y_pred_rf_reg))\n",
    "print(\"R² :\", r2_score(y_test_share, y_pred_rf_reg))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5566ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ─── 2) GradientBoosting for MVP classification ───────────────────────────\n",
    "gb_clf = GradientBoostingClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "gb_clf.fit(X_train, y_train_mvp)\n",
    "\n",
    "y_pred_gb  = gb_clf.predict(X_test)\n",
    "y_proba_gb = gb_clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(\"\\n— GradientBoostingClassifier (MVP) —\")\n",
    "print(classification_report(y_test_mvp, y_pred_gb))\n",
    "print(\"ROC‑AUC:\", roc_auc_score(y_test_mvp, y_proba_gb))\n",
    "\n",
    "\n",
    "# ─── 4) GradientBoosting for share regression ────────────────────────\n",
    "gb_reg = GradientBoostingRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "gb_reg.fit(X_train, y_train_share)\n",
    "\n",
    "y_pred_gb_reg = gb_reg.predict(X_test)\n",
    "\n",
    "print(\"\\n— GradientBoostingRegressor (Share) —\")\n",
    "print(\"MSE:\", mean_squared_error(y_test_share, y_pred_gb_reg))\n",
    "print(\"R² :\", r2_score(y_test_share, y_pred_gb_reg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c9f3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Filter to the 2022 season\n",
    "slice_2022 = test[test['Season'] == 2022].copy()\n",
    "\n",
    "# 2) Grab the top 10 by actual vote share\n",
    "top10 = slice_2022.nlargest(10, 'Share').copy()\n",
    "\n",
    "# 3) Compute RF and GB predictions\n",
    "top10['predicted_rf'] = rf_reg.predict(top10[predictors])\n",
    "top10['predicted_gb'] = gb_reg.predict(top10[predictors])\n",
    "\n",
    "# 4) Display the comparison\n",
    "print(top10[['Player', 'Share', 'predicted_rf', 'predicted_gb']].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fd1a55",
   "metadata": {},
   "source": [
    "# Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3155bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, accuracy_score, roc_auc_score,\n",
    "    mean_squared_error, r2_score\n",
    ")\n",
    "\n",
    "# 1) List your models\n",
    "models = [\n",
    "    ('Logistic Regression',           log_model, 'clf'),\n",
    "    ('Random Forest Classifier',      rf_clf,    'clf'),\n",
    "    ('Gradient Boosting Classifier',  gb_clf,    'clf'),\n",
    "    ('Linear Regression',             lin_model, 'reg'),\n",
    "    ('Ridge Regression',              ridge,     'reg'),\n",
    "    ('Random Forest Regressor',       rf_reg,    'reg'),\n",
    "    ('Gradient Boosting Regressor',   gb_reg,    'reg'),\n",
    "]\n",
    "\n",
    "# 2) Compute metrics\n",
    "rows = []\n",
    "for name, model, mtype in models:\n",
    "    r = {'Model': name}\n",
    "    if mtype == 'clf':\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:,1]\n",
    "        r.update({\n",
    "            'Precision': precision_score(y_test_mvp, y_pred, zero_division=0),\n",
    "            'Recall':    recall_score(   y_test_mvp, y_pred, zero_division=0),\n",
    "            'F1':        f1_score(       y_test_mvp, y_pred, zero_division=0),\n",
    "            'Accuracy':  accuracy_score( y_test_mvp, y_pred),\n",
    "            'ROC_AUC':   roc_auc_score(  y_test_mvp, y_proba),\n",
    "            'MSE':       np.nan,\n",
    "            'RMSE':      np.nan,\n",
    "            'R2':        np.nan,\n",
    "        })\n",
    "    else:\n",
    "        y_pred = model.predict(X_test)\n",
    "        mse  = mean_squared_error(y_test_share, y_pred)\n",
    "        r.update({\n",
    "            'Precision': np.nan,\n",
    "            'Recall':    np.nan,\n",
    "            'F1':        np.nan,\n",
    "            'Accuracy':  np.nan,\n",
    "            'ROC_AUC':   np.nan,\n",
    "            'MSE':       mse,\n",
    "            'RMSE':      np.sqrt(mse),\n",
    "            'R2':        r2_score(y_test_share, y_pred),\n",
    "        })\n",
    "    rows.append(r)\n",
    "\n",
    "# 3) Build and display DataFrame\n",
    "df_summary = pd.DataFrame(rows)\n",
    "# order columns\n",
    "df_summary = df_summary[[\n",
    "    'Model','Precision','Recall','F1','Accuracy','ROC_AUC','MSE','RMSE','R2'\n",
    "]]\n",
    "display(df_summary)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eee4b100",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBADataScraper2025:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://www.basketball-reference.com\"\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "    def get_page_content(self, url):\n",
    "        \"\"\"Fetch page content with error handling and rate limiting\"\"\"\n",
    "        try:\n",
    "            logging.info(f\"Fetching URL: {url}\")\n",
    "            time.sleep(3)  # Rate limiting\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            return BeautifulSoup(response.content, 'html.parser')\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Request error for {url}: {str(e)}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error fetching {url}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def get_basic_stats(self, year):\n",
    "        \"\"\"Extract basic statistics for a given season\"\"\"\n",
    "        url = f\"{self.base_url}/leagues/NBA_{year}_per_game.html\"\n",
    "        soup = self.get_page_content(url)\n",
    "        if not soup:\n",
    "            logging.error(f\"Failed to get basic stats page for {year}\")\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            stats_table = soup.find('table', {'id': 'per_game_stats'})\n",
    "            if not stats_table:\n",
    "                logging.error(f\"Could not find basic stats table for {year}\")\n",
    "                return None\n",
    "                \n",
    "            # Convert table to DataFrame using StringIO\n",
    "            df = pd.read_html(str(stats_table))[0]\n",
    "            \n",
    "            # Print available columns for debugging\n",
    "            logging.info(f\"Available columns for {year}: {df.columns.tolist()}\")\n",
    "            \n",
    "            # Clean up the DataFrame\n",
    "            df = df[df['Player'].notna()]  # Remove rows where Player is NaN\n",
    "            df = df[~df['Player'].str.contains('Player')]  # Remove header rows\n",
    "            \n",
    "            # Map column names to handle different naming conventions\n",
    "            column_mapping = {\n",
    "                'Tm': 'Team',\n",
    "                'Pos': 'Position',\n",
    "                'G': 'Games',\n",
    "                'MP': 'Minutes',\n",
    "                'PTS': 'Points',\n",
    "                'TRB': 'Rebounds',\n",
    "                'AST': 'Assists',\n",
    "                'STL': 'Steals',\n",
    "                'BLK': 'Blocks',\n",
    "                'TOV': 'Turnovers',\n",
    "                'FG%': 'FG_Pct',\n",
    "                '3P%': '3P_Pct',\n",
    "                'FT%': 'FT_Pct'\n",
    "            }\n",
    "            \n",
    "            # Rename columns if they exist\n",
    "            for old_col, new_col in column_mapping.items():\n",
    "                if old_col in df.columns:\n",
    "                    df = df.rename(columns={old_col: new_col})\n",
    "            \n",
    "            # Select relevant columns (using new names)\n",
    "            columns = ['Player', 'Team', 'Position', 'Season', 'Games', 'Minutes', 'Points', \n",
    "                      'Rebounds', 'Assists', 'Steals', 'Blocks', 'Turnovers', \n",
    "                      'FG_Pct', '3P_Pct', 'FT_Pct']\n",
    "            \n",
    "            # Only select columns that exist in the DataFrame\n",
    "            available_columns = [col for col in columns if col in df.columns]\n",
    "            if not available_columns:\n",
    "                logging.error(f\"No matching columns found for {year}\")\n",
    "                return None\n",
    "                \n",
    "            df = df[available_columns]\n",
    "            logging.info(f\"Successfully extracted basic stats for {year}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error extracting basic stats for {year}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def get_advanced_stats(self, year):\n",
    "        \"\"\"Extract advanced statistics for a given season\"\"\"\n",
    "        url = f\"{self.base_url}/leagues/NBA_{year}_advanced.html\"\n",
    "        soup = self.get_page_content(url)\n",
    "        if not soup:\n",
    "            logging.error(f\"Failed to get advanced stats page for {year}\")\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            # Look for the table with class 'stats_table'\n",
    "            stats_table = soup.find('table', {'class': 'stats_table'})\n",
    "            if not stats_table:\n",
    "                logging.error(f\"Could not find advanced stats table for {year}\")\n",
    "                return None\n",
    "                \n",
    "            # Convert table to DataFrame using StringIO\n",
    "            df = pd.read_html(str(stats_table))[0]\n",
    "            \n",
    "            # Print available columns for debugging\n",
    "            logging.info(f\"Available advanced columns for {year}: {df.columns.tolist()}\")\n",
    "            \n",
    "            # Clean up the DataFrame\n",
    "            df = df[df['Player'].notna()]  # Remove rows where Player is NaN\n",
    "            df = df[~df['Player'].str.contains('Player')]  # Remove header rows\n",
    "            \n",
    "            # Map column names to handle different naming conventions\n",
    "            column_mapping = {\n",
    "                'PER': 'Player_Efficiency_Rating',\n",
    "                'WS': 'Win_Shares',\n",
    "                'BPM': 'Box_Plus_Minus',\n",
    "                'USG%': 'Usage_Rate',\n",
    "                'VORP': 'Value_Over_Replacement',\n",
    "                'WS/48': 'Win_Shares_Per_48'\n",
    "            }\n",
    "            \n",
    "            # Rename columns if they exist\n",
    "            for old_col, new_col in column_mapping.items():\n",
    "                if old_col in df.columns:\n",
    "                    df = df.rename(columns={old_col: new_col})\n",
    "            \n",
    "            # Select relevant columns (using new names)\n",
    "            columns = ['Player', 'Player_Efficiency_Rating', 'Win_Shares', \n",
    "                      'Box_Plus_Minus', 'Usage_Rate', 'Value_Over_Replacement', \n",
    "                      'Win_Shares_Per_48']\n",
    "            \n",
    "            # Only select columns that exist in the DataFrame\n",
    "            available_columns = [col for col in columns if col in df.columns]\n",
    "            if not available_columns:\n",
    "                logging.error(f\"No matching advanced columns found for {year}\")\n",
    "                return None\n",
    "                \n",
    "            df = df[available_columns]\n",
    "            logging.info(f\"Successfully extracted advanced stats for {year}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error extracting advanced stats for {year}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_season(self, year):\n",
    "        \"\"\"Scrape all data for a given season\"\"\"\n",
    "        logging.info(f\"Starting to scrape data for {year} season...\")\n",
    "        \n",
    "        # Get basic stats\n",
    "        basic_stats = self.get_basic_stats(year)\n",
    "        if basic_stats is None:\n",
    "            logging.error(f\"Failed to get basic stats for {year}\")\n",
    "            return None\n",
    "            \n",
    "        # Get advanced stats\n",
    "        advanced_stats = self.get_advanced_stats(year)\n",
    "        if advanced_stats is None:\n",
    "            logging.error(f\"Failed to get advanced stats for {year}\")\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            # Merge basic and advanced stats\n",
    "            merged_stats = pd.merge(basic_stats, advanced_stats, on='Player', how='left')\n",
    "            \n",
    "            # Add Season column\n",
    "            merged_stats['Season'] = year\n",
    "            \n",
    "            logging.info(f\"Successfully merged all data for {year}\")\n",
    "            return merged_stats\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error merging data for {year}: {str(e)}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32901546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    scraper = NBADataScraper2025()\n",
    "    year = 2025  # Only scrape 2024-2025 season\n",
    "    season_data = scraper.scrape_season(year)\n",
    "    \n",
    "    if season_data is not None:\n",
    "        # Reorder columns to put Season first\n",
    "        cols = season_data.columns.tolist()\n",
    "        cols.remove('Season')\n",
    "        cols = ['Season'] + cols\n",
    "        season_data = season_data[cols]\n",
    "        \n",
    "        # Clean up the DataFrame\n",
    "        season_data = season_data.fillna(0)  # Fill missing values with 0\n",
    "        \n",
    "        # Ensure all numeric columns are float\n",
    "        numeric_columns = ['Minutes', 'Points', 'Rebounds', 'Assists', 'Steals', \n",
    "                         'Blocks', 'Turnovers', 'FG_Pct', '3P_Pct', 'FT_Pct', \n",
    "                         'Player_Efficiency_Rating', 'Win_Shares', 'Box_Plus_Minus', \n",
    "                         'Usage_Rate', 'Value_Over_Replacement', 'Win_Shares_Per_48']\n",
    "        \n",
    "        for col in numeric_columns:\n",
    "            if col in season_data.columns:\n",
    "                season_data[col] = pd.to_numeric(season_data[col], errors='coerce')\n",
    "        \n",
    "        # Save to CSV\n",
    "        output_file = 'nba_2025_season.csv'\n",
    "        season_data.to_csv(output_file, index=False)\n",
    "        logging.info(f\"Data successfully saved to {output_file}\")\n",
    "        \n",
    "        # Print some basic statistics about the dataset\n",
    "        logging.info(f\"\\nDataset Statistics:\")\n",
    "        logging.info(f\"Total number of players: {len(season_data)}\")\n",
    "        logging.info(f\"Season: {season_data['Season'].iloc[0]}\")\n",
    "    else:\n",
    "        logging.error(\"Failed to create the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5fb15b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 20:31:04,098 - INFO - Starting to scrape data for 2025 season...\n",
      "2025-05-10 20:31:04,099 - INFO - Fetching URL: https://www.basketball-reference.com/leagues/NBA_2025_per_game.html\n",
      "/var/folders/sh/qwxdyjvj1msdxcfnkpk9nz6r0000gn/T/ipykernel_63170/3320542024.py:38: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(stats_table))[0]\n",
      "2025-05-10 20:31:07,988 - INFO - Available columns for 2025: ['Rk', 'Player', 'Age', 'Team', 'Pos', 'G', 'GS', 'MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', '2P', '2PA', '2P%', 'eFG%', 'FT', 'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS', 'Awards']\n",
      "2025-05-10 20:31:07,990 - INFO - Successfully extracted basic stats for 2025\n",
      "2025-05-10 20:31:07,991 - INFO - Fetching URL: https://www.basketball-reference.com/leagues/NBA_2025_advanced.html\n",
      "/var/folders/sh/qwxdyjvj1msdxcfnkpk9nz6r0000gn/T/ipykernel_63170/3320542024.py:103: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(stats_table))[0]\n",
      "2025-05-10 20:31:12,116 - INFO - Available advanced columns for 2025: ['Rk', 'Player', 'Age', 'Team', 'Pos', 'G', 'GS', 'MP', 'PER', 'TS%', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'OWS', 'DWS', 'WS', 'WS/48', 'OBPM', 'DBPM', 'BPM', 'VORP', 'Awards']\n",
      "2025-05-10 20:31:12,119 - INFO - Successfully extracted advanced stats for 2025\n",
      "2025-05-10 20:31:12,120 - INFO - Successfully merged all data for 2025\n",
      "2025-05-10 20:31:12,131 - INFO - Data successfully saved to nba_2025_season.csv\n",
      "2025-05-10 20:31:12,131 - INFO - \n",
      "Dataset Statistics:\n",
      "2025-05-10 20:31:12,131 - INFO - Total number of players: 1246\n",
      "2025-05-10 20:31:12,132 - INFO - Season: 2025\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
