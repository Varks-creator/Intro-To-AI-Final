{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70b768b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84089e3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Set up logging with more detailed format\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97233189",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class NBADataScraper:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://www.basketball-reference.com\"\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "    def get_page_content(self, url):\n",
    "        \"\"\"Fetch page content with error handling and rate limiting\"\"\"\n",
    "        try:\n",
    "            logging.info(f\"Fetching URL: {url}\")\n",
    "            time.sleep(3)  # Rate limiting\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            return BeautifulSoup(response.content, 'html.parser')\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Request error for {url}: {str(e)}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error fetching {url}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def get_mvp_winner(self, year):\n",
    "        \"\"\"Extract MVP winner for a given season\"\"\"\n",
    "        url = f\"{self.base_url}/awards/awards_{year}.html\"\n",
    "        soup = self.get_page_content(url)\n",
    "        if not soup:\n",
    "            logging.error(f\"Failed to get MVP page for {year}\")\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            mvp_table = soup.find('table', {'id': 'mvp'})\n",
    "            if not mvp_table:\n",
    "                logging.error(f\"Could not find MVP table for {year}\")\n",
    "                return None\n",
    "                \n",
    "            # Get the first row (winner)\n",
    "            winner_row = mvp_table.find('tbody').find('tr')\n",
    "            if not winner_row:\n",
    "                logging.error(f\"Could not find MVP winner row for {year}\")\n",
    "                return None\n",
    "                \n",
    "            player_name = winner_row.find('td', {'data-stat': 'player'}).text.strip()\n",
    "            logging.info(f\"Found MVP winner for {year}: {player_name}\")\n",
    "            return player_name\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error extracting MVP winner for {year}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def get_basic_stats(self, year):\n",
    "        \"\"\"Extract basic statistics for a given season\"\"\"\n",
    "        url = f\"{self.base_url}/leagues/NBA_{year}_per_game.html\"\n",
    "        soup = self.get_page_content(url)\n",
    "        if not soup:\n",
    "            logging.error(f\"Failed to get basic stats page for {year}\")\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            stats_table = soup.find('table', {'id': 'per_game_stats'})\n",
    "            if not stats_table:\n",
    "                logging.error(f\"Could not find basic stats table for {year}\")\n",
    "                return None\n",
    "                \n",
    "            # Convert table to DataFrame\n",
    "            df = pd.read_html(str(stats_table))[0]\n",
    "            \n",
    "            # Print available columns for debugging\n",
    "            logging.info(f\"Available columns for {year}: {df.columns.tolist()}\")\n",
    "            \n",
    "            # Clean up the DataFrame\n",
    "            df = df[df['Player'].notna()]  # Remove rows where Player is NaN\n",
    "            df = df[~df['Player'].str.contains('Player')]  # Remove header rows\n",
    "            \n",
    "            # Map column names to handle different naming conventions\n",
    "            column_mapping = {\n",
    "                'Tm': 'Team',\n",
    "                'Pos': 'Position',\n",
    "                'G': 'Games',\n",
    "                'MP': 'Minutes',\n",
    "                'PTS': 'Points',\n",
    "                'TRB': 'Rebounds',\n",
    "                'AST': 'Assists',\n",
    "                'STL': 'Steals',\n",
    "                'BLK': 'Blocks',\n",
    "                'TOV': 'Turnovers',\n",
    "                'FG%': 'FG_Pct',\n",
    "                '3P%': '3P_Pct',\n",
    "                'FT%': 'FT_Pct'\n",
    "            }\n",
    "            \n",
    "            # Rename columns if they exist\n",
    "            for old_col, new_col in column_mapping.items():\n",
    "                if old_col in df.columns:\n",
    "                    df = df.rename(columns={old_col: new_col})\n",
    "            \n",
    "            # Select relevant columns (using new names)\n",
    "            columns = ['Player', 'Team', 'Position', 'Games', 'Minutes', 'Points', \n",
    "                      'Rebounds', 'Assists', 'Steals', 'Blocks', 'Turnovers', \n",
    "                      'FG_Pct', '3P_Pct', 'FT_Pct']\n",
    "            \n",
    "            # Only select columns that exist in the DataFrame\n",
    "            available_columns = [col for col in columns if col in df.columns]\n",
    "            if not available_columns:\n",
    "                logging.error(f\"No matching columns found for {year}\")\n",
    "                return None\n",
    "                \n",
    "            df = df[available_columns]\n",
    "            logging.info(f\"Successfully extracted basic stats for {year}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error extracting basic stats for {year}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def get_advanced_stats(self, year):\n",
    "        \"\"\"Extract advanced statistics for a given season\"\"\"\n",
    "        url = f\"{self.base_url}/leagues/NBA_{year}_advanced.html\"\n",
    "        soup = self.get_page_content(url)\n",
    "        if not soup:\n",
    "            logging.error(f\"Failed to get advanced stats page for {year}\")\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            # Look for the table with class 'stats_table'\n",
    "            stats_table = soup.find('table', {'class': 'stats_table'})\n",
    "            if not stats_table:\n",
    "                logging.error(f\"Could not find advanced stats table for {year}\")\n",
    "                return None\n",
    "                \n",
    "            # Convert table to DataFrame\n",
    "            df = pd.read_html(str(stats_table))[0]\n",
    "            \n",
    "            # Print available columns for debugging\n",
    "            logging.info(f\"Available advanced columns for {year}: {df.columns.tolist()}\")\n",
    "            \n",
    "            # Clean up the DataFrame\n",
    "            df = df[df['Player'].notna()]  # Remove rows where Player is NaN\n",
    "            df = df[~df['Player'].str.contains('Player')]  # Remove header rows\n",
    "            \n",
    "            # Map column names to handle different naming conventions\n",
    "            column_mapping = {\n",
    "                'PER': 'Player_Efficiency_Rating',\n",
    "                'WS': 'Win_Shares',\n",
    "                'BPM': 'Box_Plus_Minus',\n",
    "                'USG%': 'Usage_Rate',\n",
    "                'VORP': 'Value_Over_Replacement',\n",
    "                'WS/48': 'Win_Shares_Per_48'\n",
    "            }\n",
    "            \n",
    "            # Rename columns if they exist\n",
    "            for old_col, new_col in column_mapping.items():\n",
    "                if old_col in df.columns:\n",
    "                    df = df.rename(columns={old_col: new_col})\n",
    "            \n",
    "            # Select relevant columns (using new names)\n",
    "            columns = ['Player', 'Player_Efficiency_Rating', 'Win_Shares', \n",
    "                      'Box_Plus_Minus', 'Usage_Rate', 'Value_Over_Replacement', \n",
    "                      'Win_Shares_Per_48']\n",
    "            \n",
    "            # Only select columns that exist in the DataFrame\n",
    "            available_columns = [col for col in columns if col in df.columns]\n",
    "            if not available_columns:\n",
    "                logging.error(f\"No matching advanced columns found for {year}\")\n",
    "                return None\n",
    "                \n",
    "            df = df[available_columns]\n",
    "            logging.info(f\"Successfully extracted advanced stats for {year}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error extracting advanced stats for {year}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_season(self, year):\n",
    "        \"\"\"Scrape all data for a given season\"\"\"\n",
    "        logging.info(f\"Starting to scrape data for {year} season...\")\n",
    "        \n",
    "        # Get MVP winner\n",
    "        mvp_winner = self.get_mvp_winner(year)\n",
    "        if mvp_winner is None:\n",
    "            logging.error(f\"Failed to get MVP winner for {year}\")\n",
    "            return None\n",
    "        \n",
    "        # Get basic stats\n",
    "        basic_stats = self.get_basic_stats(year)\n",
    "        if basic_stats is None:\n",
    "            logging.error(f\"Failed to get basic stats for {year}\")\n",
    "            return None\n",
    "            \n",
    "        # Get advanced stats\n",
    "        advanced_stats = self.get_advanced_stats(year)\n",
    "        if advanced_stats is None:\n",
    "            logging.error(f\"Failed to get advanced stats for {year}\")\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            # Merge basic and advanced stats\n",
    "            merged_stats = pd.merge(basic_stats, advanced_stats, on='Player', how='left')\n",
    "            \n",
    "            # Add MVP column\n",
    "            merged_stats['MVP'] = merged_stats['Player'].apply(lambda x: 1 if x == mvp_winner else 0)\n",
    "            \n",
    "            logging.info(f\"Successfully merged all data for {year}\")\n",
    "            return merged_stats\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error merging data for {year}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_all_seasons(self, start_year=1981, end_year=2024):\n",
    "        \"\"\"Scrape data for all seasons in the given range\"\"\"\n",
    "        all_data = []\n",
    "        \n",
    "        for year in tqdm(range(start_year, end_year + 1), desc=\"Scraping seasons\"):\n",
    "            season_data = self.scrape_season(year)\n",
    "            if season_data is not None:\n",
    "                all_data.append(season_data)\n",
    "                logging.info(f\"Successfully scraped {year} season\")\n",
    "            else:\n",
    "                logging.error(f\"Failed to scrape {year} season\")\n",
    "                \n",
    "        if not all_data:\n",
    "            logging.error(\"No data was collected!\")\n",
    "            return None\n",
    "            \n",
    "        # Combine all seasons\n",
    "        final_df = pd.concat(all_data, ignore_index=True)\n",
    "        \n",
    "        # Clean up the final DataFrame\n",
    "        final_df = final_df.fillna(0)  # Fill missing values with 0\n",
    "        \n",
    "        # Ensure all numeric columns are float\n",
    "        numeric_columns = ['Minutes', 'Points', 'Rebounds', 'Assists', 'Steals', \n",
    "                         'Blocks', 'Turnovers', 'FG_Pct', '3P_Pct', 'FT_Pct', \n",
    "                         'Player_Efficiency_Rating', 'Win_Shares', 'Box_Plus_Minus', \n",
    "                         'Usage_Rate', 'Value_Over_Replacement', 'Win_Shares_Per_48']\n",
    "        \n",
    "        for col in numeric_columns:\n",
    "            if col in final_df.columns:\n",
    "                final_df[col] = pd.to_numeric(final_df[col], errors='coerce')\n",
    "        \n",
    "        return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc76bc3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    scraper = NBADataScraper()\n",
    "    final_df = scraper.scrape_all_seasons()\n",
    "    \n",
    "    if final_df is not None:\n",
    "        # Save to CSV\n",
    "        output_file = 'nba_mvp_data.csv'\n",
    "        final_df.to_csv(output_file, index=False)\n",
    "        logging.info(f\"Data successfully saved to {output_file}\")\n",
    "        \n",
    "        # Print some basic statistics about the dataset\n",
    "        logging.info(f\"\\nDataset Statistics:\")\n",
    "        logging.info(f\"Total number of player-seasons: {len(final_df)}\")\n",
    "        logging.info(f\"Number of MVP winners: {final_df['MVP'].sum()}\")\n",
    "        logging.info(f\"Seasons covered: {final_df['Season'].min()} to {final_df['Season'].max()}\")\n",
    "    else:\n",
    "        logging.error(\"Failed to create the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d595a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
